{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14109750,"sourceType":"datasetVersion","datasetId":7235950},{"sourceId":677712,"sourceType":"modelInstanceVersion","modelInstanceId":513931,"modelId":528570},{"sourceId":678636,"sourceType":"modelInstanceVersion","modelInstanceId":514712,"modelId":529359},{"sourceId":682649,"sourceType":"modelInstanceVersion","modelInstanceId":518018,"modelId":532626},{"sourceId":682717,"sourceType":"modelInstanceVersion","modelInstanceId":518068,"modelId":532667},{"sourceId":687886,"sourceType":"modelInstanceVersion","modelInstanceId":521662,"modelId":535820},{"sourceId":687892,"sourceType":"modelInstanceVersion","modelInstanceId":521667,"modelId":535825},{"sourceId":690108,"sourceType":"modelInstanceVersion","modelInstanceId":523129,"modelId":537138},{"sourceId":691092,"sourceType":"modelInstanceVersion","modelInstanceId":523963,"modelId":537976}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1D CNN-Attention AutoEncoder - Prokaryotic Domain","metadata":{"id":"bhqFtNSBquxv"}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport time\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport math\nimport torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\nfrom torchvision import transforms\nfrom tqdm.notebook import tqdm\n\n!pip install blosc\nimport blosc\nimport zstandard as zstd\n\n\n#------- Parameters ----------\n\nP = 4 #latent_size\nbatch_size = 128\nchunk_size = 1280","metadata":{"id":"Uf9JgHXvwrbm","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T10:16:09.50999Z","iopub.execute_input":"2025-12-18T10:16:09.510547Z","iopub.status.idle":"2025-12-18T10:16:12.667457Z","shell.execute_reply.started":"2025-12-18T10:16:09.510526Z","shell.execute_reply":"2025-12-18T10:16:12.666613Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.manual_seed(0)\ntorch.cuda.manual_seed(0)\nnp.random.seed(0)\nrandom.seed(0)","metadata":{"id":"a7ZBCDWknqiV","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T10:16:12.668982Z","iopub.execute_input":"2025-12-18T10:16:12.66921Z","iopub.status.idle":"2025-12-18T10:16:12.674515Z","shell.execute_reply.started":"2025-12-18T10:16:12.669188Z","shell.execute_reply":"2025-12-18T10:16:12.67402Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training Function","metadata":{}},{"cell_type":"code","source":"def train(model,\n          train_loader,\n          test_loader,\n          training_iterations,\n          evaluation_iterations,\n          verbose=False,\n          device=None):\n\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    model = model.to(device)\n    model.train()\n\n    optimizer = optim.NAdam(model.parameters(), lr=0.0005)\n\n    scaler = torch.cuda.amp.GradScaler()\n\n    train_loss_vals = []\n    eval_loss_vals = []\n\n    train_losses = []\n    eval_losses = []\n\n    train_accuracies = []\n    eval_accuracies = []\n\n    pbar = tqdm(range(training_iterations))\n    step_counter = 0\n\n    while step_counter < training_iterations:\n        for images, labels in train_loader:\n            images = images.to(device)\n\n        \n            with torch.cuda.amp.autocast(dtype=torch.float16):\n                encoded, reconstruction = model(images)\n                mse_loss = torch.mean((images - reconstruction) ** 2)\n\n            train_loss_vals.append(mse_loss.item())\n\n            optimizer.zero_grad()\n\n      \n            scaler.scale(mse_loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            \n            if step_counter % evaluation_iterations == 0:\n                model.eval()\n                eval_loss_batch = []\n\n                with torch.no_grad():\n                    for images, labels in test_loader:\n                        images = images.to(device)\n\n                        with torch.cuda.amp.autocast(dtype=torch.float16):\n                            _, reconstruction = model(images)\n                            loss_eval = torch.mean((images - reconstruction) ** 2)\n\n                        eval_loss_batch.append(loss_eval.item())\n\n                \n                mean_train_loss = np.mean(train_loss_vals)\n                mean_eval_loss = np.mean(eval_loss_batch)\n\n                train_losses.append(mean_train_loss)\n                eval_losses.append(mean_eval_loss)\n\n                \n                train_acc_batch = []\n                with torch.no_grad():\n                    for images, labels in train_loader:\n                        images = images.to(device)\n\n                        with torch.cuda.amp.autocast(dtype=torch.float16):\n                            _, reconstruction = model(images)\n\n                       \n                        recon = reconstruction.float()\n                        imgs = images.float()\n\n                        pred = torch.clamp(torch.round(recon * 4.0), 0, 4).long()\n                        true = torch.clamp(torch.round(imgs * 4.0), 0, 4).long()\n\n                        train_acc_batch.append((pred == true).float().mean().item())\n\n                train_acc = np.mean(train_acc_batch) * 100\n\n                eval_acc_batch = []\n                with torch.no_grad():\n                    for images, labels in test_loader:\n                        images = images.to(device)\n\n                        with torch.cuda.amp.autocast(dtype=torch.float16):\n                            _, reconstruction = model(images)\n\n                        recon = reconstruction.float()\n                        imgs = images.float()\n\n                        pred = torch.clamp(torch.round(recon * 4.0), 0, 4).long()\n                        true = torch.clamp(torch.round(imgs * 4.0), 0, 4).long()\n\n                        eval_acc_batch.append((pred == true).float().mean().item())\n\n                eval_acc = np.mean(eval_acc_batch) * 100\n\n                train_accuracies.append(train_acc)\n                eval_accuracies.append(eval_acc)\n\n                if verbose:\n                    tqdm.write(\n                        f\"Step {step_counter}: \"\n                        f\"Train Loss = {mean_train_loss:.6f}, \"\n                        f\"Eval Loss = {mean_eval_loss:.6f}, \"\n                        f\"Train Accuracy = {train_acc:.2f}%, \"\n                        f\"Eval Accuracy = {eval_acc:.2f}%\"\n                    )\n\n                train_loss_vals = []\n                eval_loss_vals = []\n                model.train()\n\n            step_counter += 1\n            pbar.update(1)\n\n            if step_counter >= training_iterations:\n                print(\"Completed Training:\")\n                break\n\n    print(f\"\\nFinal Training Loss: {train_losses[-1]:.6f}\")\n    print(f\"Final Evaluation Loss: {eval_losses[-1]:.6f}\")\n    print(f\"Final Training Accuracy: {train_accuracies[-1]:.6f}%\")\n    print(f\"Final Evaluation Accuracy: {eval_accuracies[-1]:.6f}%\")\n\n    return model, train_losses, eval_losses, train_accuracies, eval_accuracies","metadata":{"id":"bea5i2lpQEdq","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T10:16:12.675284Z","iopub.execute_input":"2025-12-18T10:16:12.675544Z","iopub.status.idle":"2025-12-18T10:16:12.691037Z","shell.execute_reply.started":"2025-12-18T10:16:12.675529Z","shell.execute_reply":"2025-12-18T10:16:12.690316Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Architecture","metadata":{}},{"cell_type":"code","source":"# Self-Attention Block \nclass _SelfAttnBlock1D(nn.Module):\n    def __init__(self, in_channels: int, embed_dim: int,\n                 num_heads: int = 4, dropout: float = 0.1):\n        super().__init__()\n        assert embed_dim % num_heads == 0\n\n        self.proj_in = nn.Conv1d(in_channels, embed_dim, kernel_size=1)\n\n        self.attn = nn.MultiheadAttention(\n            embed_dim=embed_dim,\n            num_heads=num_heads,\n            batch_first=True,\n            dropout=dropout\n        )\n\n        self.ln1 = nn.LayerNorm(embed_dim)\n\n        self.ffn = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.GELU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n\n        self.ln2 = nn.LayerNorm(embed_dim)\n\n    @staticmethod\n    def _pos_enc(L, C, device, dtype):\n        pe = torch.zeros(L, C, device=device, dtype=dtype)\n        pos = torch.arange(0, L, device=device, dtype=dtype).unsqueeze(1)\n        div = torch.exp(torch.arange(0, C, 2, device=device, dtype=dtype)\n                        * (-math.log(10000.0) / C))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        return pe.unsqueeze(0)\n\n    def forward(self, x):\n        B, C, L = x.shape\n        h = self.proj_in(x)            \n        h = h.permute(0, 2, 1)        \n        pe = self._pos_enc(L, h.size(-1), x.device, h.dtype)\n        h = h + pe\n\n        attn_out, _ = self.attn(h, h, h, need_weights=False)\n        h = self.ln1(h + attn_out)\n\n        ffn_out = self.ffn(h)\n        h = self.ln2(h + ffn_out)\n\n        return h.permute(0, 2, 1)     \n\n\n# Decoder with Self-Attention \nclass _DecoderWithSelfAttention(nn.Module):\n    def __init__(self, latent_dim, embed_dim=32, num_heads=4):\n        super().__init__()\n\n        # L/16 -> L/8\n        self.up1 = nn.ConvTranspose1d(latent_dim, 128, 3, 2, 1, 1)\n        self.bn1 = nn.BatchNorm1d(128)\n        self.attn1 = _SelfAttnBlock1D(128, 128, num_heads)\n\n        # L/8 -> L/4\n        self.up2 = nn.ConvTranspose1d(128, 64, 3, 2, 1, 1)\n        self.bn2 = nn.BatchNorm1d(64)\n        self.attn2 = _SelfAttnBlock1D(64, 64, num_heads)\n\n        # L/4 -> L/2\n        self.up3 = nn.ConvTranspose1d(64, embed_dim, 3, 2, 1, 1)\n        self.bn3 = nn.BatchNorm1d(embed_dim)\n        self.attn3 = _SelfAttnBlock1D(embed_dim, embed_dim, num_heads)\n\n        # L/2 -> L\n        self.up_final = nn.ConvTranspose1d(embed_dim, 1, 3, 2, 1, 1)\n\n    def forward(self, z):\n        x = F.relu(self.bn1(self.up1(z)))\n        x = self.attn1(x)\n\n        x = F.relu(self.bn2(self.up2(x)))\n        x = self.attn2(x)\n\n        x = F.relu(self.bn3(self.up3(x)))\n        x = self.attn3(x)\n\n        return torch.sigmoid(self.up_final(x))\n\n\n\n# Full Autoencoder \nclass ConvolutionalAutoEncoder1D(nn.Module):\n    def __init__(self, in_channels=1, latent_dim=4,\n                 attn_embed_dim=32, attn_heads=4):\n        super().__init__()\n\n        self.latent_dim = latent_dim\n\n        self.encoder_stem = nn.Sequential(\n            nn.Conv1d(in_channels, 32, 3, 2, 1),\n            nn.BatchNorm1d(32),\n            nn.ReLU(),\n\n            nn.Conv1d(32, 64, 3, 2, 1),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n\n            nn.Conv1d(64, 128, 3, 2, 1),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n\n            \n            nn.Conv1d(128, 128, 3, 2, 1),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n        )\n\n        \n        self.encoder_attn = _SelfAttnBlock1D(\n            in_channels=128,\n            embed_dim=128,\n            num_heads=attn_heads\n        )\n\n    \n        self.channel_funnel = nn.Sequential(\n            nn.Conv1d(128, 64, kernel_size=1),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n\n            nn.Conv1d(64, 32, kernel_size=1),\n            nn.BatchNorm1d(32),\n            nn.ReLU(),\n\n            nn.Conv1d(32, latent_dim, kernel_size=1),\n        )\n\n        \n        self.latent_to_decoder = nn.Conv1d(\n            latent_dim, attn_embed_dim, kernel_size=1\n        )\n\n        self.decoder = _DecoderWithSelfAttention(\n            latent_dim=attn_embed_dim,\n            embed_dim=attn_embed_dim,\n            num_heads=attn_heads\n        )\n\n    def forward_enc(self, x):\n        h = self.encoder_stem(x)     \n        h = self.encoder_attn(h)     \n        z = self.channel_funnel(h) \n        return z\n\n    def forward_dec(self, z):\n        z = self.latent_to_decoder(z)\n        return self.decoder(z)\n\n    def forward(self, x):\n        z = self.forward_enc(x)\n        y = self.forward_dec(z)\n        return z.to(torch.float16), y.to(torch.float16)","metadata":{"id":"KGkGg8J54Bke","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T10:16:12.691836Z","iopub.execute_input":"2025-12-18T10:16:12.692001Z","iopub.status.idle":"2025-12-18T10:16:12.709642Z","shell.execute_reply.started":"2025-12-18T10:16:12.691988Z","shell.execute_reply":"2025-12-18T10:16:12.708843Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Instantiation","metadata":{}},{"cell_type":"code","source":"conv1d_model = ConvolutionalAutoEncoder1D(in_channels=1, latent_dim=P,\n                                          attn_embed_dim=32, attn_heads=4)","metadata":{"id":"AkA9BYGN5tg-","outputId":"9ee8b3c0-115e-4e38-ed00-f41a1f19a5eb","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T10:16:12.711859Z","iopub.execute_input":"2025-12-18T10:16:12.712135Z","iopub.status.idle":"2025-12-18T10:16:12.737667Z","shell.execute_reply.started":"2025-12-18T10:16:12.712113Z","shell.execute_reply":"2025-12-18T10:16:12.737175Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataloaders and Model Training","metadata":{}},{"cell_type":"code","source":"history_per_file = {}\nfile_times = {}\n\nfiles_to_load = ['AeCa', 'HaHi', 'EsCo']\ndata_dir = \"/kaggle/input/dnacorpus/DNACorpus/Prokaryotic\"\n\nfor fname in files_to_load:\n    print(f\"\\n=== Training on {fname} ===\")\n    \n    start_time = time.time()   \n\n    with open(os.path.join(data_dir, fname), 'r') as f:\n        seq = f.read().strip()\n\n    content = seq.translate(str.maketrans('ATCG', '0123'))\n\n    remainder = len(content) % chunk_size\n    if remainder != 0:\n        pad_len = chunk_size - remainder\n        content = content.ljust(len(content) + pad_len, '4')\n    \n    content_array = np.frombuffer(content.encode('ascii'), dtype=np.uint8) - ord('0')\n    content_array = content_array.astype(np.float16) / 4.0\n\n    reshaped = content_array.reshape(-1, chunk_size)\n    tensor_data = torch.tensor(reshaped, dtype=torch.float16).unsqueeze(1)\n\n    dummy_labels = torch.zeros(len(tensor_data), dtype=torch.uint8)\n\n    full_dataset = TensorDataset(tensor_data, dummy_labels)\n    train_size = int(0.8 * len(full_dataset))\n    test_size = len(full_dataset) - train_size\n    train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    #Train model\n    conv1d_model, train_losses, eval_losses, train_accs, eval_accs = train(\n        conv1d_model,\n        train_loader,\n        test_loader,\n        training_iterations=10000,\n        evaluation_iterations=200,\n        verbose=True\n    )\n\n    end_time = time.time()\n    file_times[fname] = end_time - start_time\n    history_per_file[fname] = {\n    \"train_loss\": train_losses,\n    \"val_loss\": eval_losses,\n    \"train_acc\": train_accs,\n    \"val_acc\": eval_accs\n    }\n\nprint(\"\\n\\n PER-FILE TRAINING TIME \")\n\nfor fname in files_to_load:\n    print(f\"\\nFile: {fname}\")\n    print(f\"  Training Time: {file_times[fname]:.2f} seconds\")\n\nprint(\"\\n LOSS AND ACCURACY \")\n\nplt.figure(figsize=(16, 4))\n\nfor i, fname in enumerate(files_to_load):\n    hist = history_per_file[fname]\n    plt.subplot(1, len(files_to_load), i + 1)\n    plt.plot(hist[\"train_loss\"], label=\"Train\")\n    plt.plot(hist[\"val_loss\"], label=\"Val\")\n    plt.title(f\"{fname} - Loss\")\n    plt.xlabel(\"Iterations\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\nplt.figure(figsize=(16, 4))\n\nfor i, fname in enumerate(files_to_load):\n    hist = history_per_file[fname]\n    plt.subplot(1, len(files_to_load), i + 1)\n    plt.plot(hist[\"train_acc\"], label=\"Train\")\n    plt.plot(hist[\"val_acc\"], label=\"Val\")\n    plt.title(f\"{fname} - Accuracy\")\n    plt.xlabel(\"Iterations\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.grid(True)\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"Ll17EURowwBF","outputId":"cd00c69f-a15f-4ea7-ee71-fe7191d93460","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T10:16:12.738397Z","iopub.execute_input":"2025-12-18T10:16:12.738617Z","iopub.status.idle":"2025-12-18T10:16:12.998567Z","shell.execute_reply.started":"2025-12-18T10:16:12.738602Z","shell.execute_reply":"2025-12-18T10:16:12.997846Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Saving Model Weights","metadata":{}},{"cell_type":"code","source":"# Save model weights \ntorch.save(conv1d_model.state_dict(), \"/kaggle/working/autoencoder.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T10:16:12.999359Z","iopub.execute_input":"2025-12-18T10:16:12.999701Z","iopub.status.idle":"2025-12-18T10:16:13.013896Z","shell.execute_reply.started":"2025-12-18T10:16:12.999656Z","shell.execute_reply":"2025-12-18T10:16:13.01336Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Testing","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = ConvolutionalAutoEncoder1D(in_channels=1, latent_dim=P)\nmodel.load_state_dict(torch.load(\"/kaggle/working/autoencoder.pth\", map_location=device))\n# to load already trained model\n# model.load_state_dict(torch.load(\"/kaggle/input/fp16-improved-prokar-p4-20k/pytorch/default/1/FP16_Improved_Prokar_P4_20K.pth\", map_location=device))  \nmodel.to(device)\nmodel.eval()\nmodel.half()\n\nlatent_dir = \"/kaggle/working/latent\"\nos.makedirs(latent_dir, exist_ok=True)  \n\nchunk_length = k\ncorpus_dir = \"/kaggle/input/dnacorpus/DNACorpus/Prokaryotic\"\nvocab = ['A', 'T', 'C', 'G', 'N']\n\ncomp_times = []\ndecomp_times = []\nfile_labels = []\n\ndef preprocess_sequence_1d(seq, chunk_length=k):\n    seq = seq.translate(str.maketrans(\"ATCG\", \"0123\"))\n    remainder = len(seq) % chunk_length\n    if remainder != 0:\n        pad_len = chunk_length - remainder\n        seq = seq.ljust(len(seq) + pad_len, '4')\n    arr = np.frombuffer(seq.encode('ascii'), dtype=np.uint8) - ord(\"0\")\n    arr = arr / 4.0 \n    arr = np.reshape(arr, (-1, chunk_length))\n    return torch.tensor(arr, dtype=torch.float16).unsqueeze(1)  \n\ndef decode_tensor_to_seq_1d(tensor, vocab):\n    arr = tensor.squeeze().cpu().numpy()\n    arr = np.clip(arr, 0, 1)\n    indices = np.rint(arr * (len(vocab) - 1)).astype(int)\n    indices = np.clip(indices, 0, len(vocab) - 1)\n    return ''.join([vocab[i] for i in indices.flatten()])\n\ndef compute_accuracy(seq1, seq2):\n    matches = sum(a == b for a, b in zip(seq1, seq2))\n    return matches / len(seq1) * 100\n\n# train_files \ntrain_files = ['AeCa', 'HaHi', 'EsCo']\n\ntest_files = [f for f in os.listdir(corpus_dir) if f not in train_files]\n\nresults = []\n\nfor filename in sorted(os.listdir(corpus_dir)):\n    filepath = os.path.join(corpus_dir, filename)\n    if not os.path.isfile(filepath):\n        continue\n\n    with open(filepath, 'r') as f:\n        raw_seq = f.read().strip().upper()\n\n    print(f\"\\nProcessing: {filename} | Length: {len(raw_seq)}\")\n    input_tensor = preprocess_sequence_1d(raw_seq, chunk_length).to(device)\n\n    recon_seq = \"\"\n    latent_total = 0\n    n_chunks = len(input_tensor)\n\n    with torch.no_grad():\n        first_chunk = model.forward_enc(input_tensor[0].unsqueeze(0))\n        latent_shape = first_chunk.shape[1:]\n        latent_dtype = np.float16\n\n        mmap_path = os.path.join(latent_dir, f\"{filename}.dat\")\n        latent_mmap = np.memmap(mmap_path, dtype=latent_dtype, mode='w+', shape=(n_chunks, *latent_shape))\n\n        comp_start = time.time()\n        decomp_time_accum = 0.0\n\n        for i in tqdm(range(n_chunks), desc=f\"  → Encoding & Decoding {filename}\"):\n            chunk = input_tensor[i].unsqueeze(0)\n    \n            \n            t1 = time.time()\n            encoded = model.forward_enc(chunk)\n            t2 = time.time()\n            comp_time_step = (t2 - t1)\n\n            \n            t3 = time.time()\n            decoded = model.forward_dec(encoded)\n            t4 = time.time()\n            decomp_time_step = (t4 - t3)\n\n            decomp_time_accum += decomp_time_step\n\n            latent_np = encoded.cpu().numpy().astype(latent_dtype)[0]\n            latent_mmap[i] = latent_np\n\n            latent_total += encoded.numel()\n            recon_seq += decode_tensor_to_seq_1d(decoded, vocab)\n\n      \n\n        latent_mmap.flush()\n        final_latent = np.memmap(mmap_path, dtype=latent_dtype, mode='r', shape=(n_chunks, *latent_shape))\n        np.save(os.path.join(latent_dir, f\"{filename}.npy\"), np.array(final_latent))\n        os.remove(mmap_path)\n\n    recon_seq = recon_seq[:len(raw_seq)]\n    accuracy = compute_accuracy(raw_seq, recon_seq)\n\n    comp_end = time.time()\n\n    comp_times.append(comp_end - comp_start)\n    decomp_times.append(decomp_time_accum)\n    file_labels.append(filename)\n\n\n    results.append({\n        \"file\": filename,\n        \"original_len\": len(raw_seq),\n        \"latent_size\": latent_total * 2,\n        \"accuracy (%)\": round(accuracy, 2),\n        \"compress_ratio\": round(latent_total * 2 / len(raw_seq), 2)\n    })\n\n\ndf = pd.DataFrame(results)\n\ndf_train = df[df[\"file\"].isin(train_files)]\ndf_train.to_csv(\"/kaggle/working/evaluation_results.csv\", index=False)\nprint(\"\\nSaved evaluation_results.csv (TRAIN files only)\")\nprint(df_train.to_string(index=False))\n\n\ndf_test = df[df[\"file\"].isin(test_files)]\ndf_test.to_csv(\"/kaggle/working/evaluation_results_test.csv\", index=False)\nprint(\"\\nSaved evaluation_results_test.csv (TEST files only)\")\nprint(df_test.to_string(index=False))\n\nplt.figure(figsize=(14, 5))\n\nplt.plot(file_labels, comp_times, marker='o', label=\"CT\")\nplt.plot(file_labels, decomp_times, marker='o', label=\"DT\")\n\nplt.xlabel(\"Files\")\nplt.ylabel(\"Time (s)\")\nplt.title(\"Compression vs Decompression Time per File\")\nplt.xticks(rotation=45)\nplt.legend()\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T10:16:13.014542Z","iopub.execute_input":"2025-12-18T10:16:13.014779Z","iopub.status.idle":"2025-12-18T10:17:07.574607Z","shell.execute_reply.started":"2025-12-18T10:16:13.014762Z","shell.execute_reply":"2025-12-18T10:17:07.57391Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Int8 Quantization ","metadata":{}},{"cell_type":"code","source":"quant_results = []\n\nrecon_int8_dir  = \"/kaggle/working/recon_int8\"\nlatent_int8_dir = \"/kaggle/working/latent_int8\"\n\nos.makedirs(recon_int8_dir, exist_ok=True)\nos.makedirs(latent_int8_dir, exist_ok=True)\n\ndef quantize_int8_symmetric_per_channel(latent_fp16):\n    \n    scale = np.max(np.abs(latent_fp16), axis=(0, 2), keepdims=True)\n    scale[scale == 0] = 1.0  \n\n    latent_int8 = np.round(latent_fp16 / scale * 127).astype(np.int8)\n    return latent_int8, scale.astype(np.float16)\n\n\ndef dequantize_int8_symmetric_per_channel(latent_int8, scale):\n   \n    return latent_int8.astype(np.float16) * scale / 127\n\n\n\nfor _, row in df.iterrows():\n    fname = row[\"file\"]\n\n    with open(os.path.join(corpus_dir, fname), \"r\") as f:\n        raw_seq = f.read().strip().upper()\n\n    latent_fp16 = np.load(os.path.join(latent_dir, f\"{fname}.npy\"))\n\n    latent_int8, scale = quantize_int8_symmetric_per_channel(latent_fp16)\n\n    np.save(os.path.join(latent_int8_dir, f\"{fname}_latent_int8.npy\"), latent_int8)\n    np.save(os.path.join(latent_int8_dir, f\"{fname}_scale.npy\"), scale)\n\n    latent_fp16_recon = dequantize_int8_symmetric_per_channel(latent_int8, scale)\n    latent_tensor = torch.tensor(latent_fp16_recon,\n                                 dtype=torch.float16,\n                                 device=device)\n\n    recon_seq = \"\"\n    with torch.no_grad():\n        for i in tqdm(range(latent_tensor.shape[0]),\n                      desc=f\"  → Decoding int8 {fname}\"):\n            decoded_chunk = model.forward_dec(latent_tensor[i:i+1])\n            recon_seq += decode_tensor_to_seq_1d(decoded_chunk, vocab)\n\n    recon_seq = recon_seq[:len(raw_seq)]\n\n    if fname in train_files:\n        subdir = os.path.join(recon_int8_dir, \"train\")\n    else:\n        subdir = os.path.join(recon_int8_dir, \"test\")\n\n    os.makedirs(subdir, exist_ok=True)\n\n    out_path = os.path.join(subdir, f\"{fname}_recon_int8.txt\")\n    with open(out_path, \"w\") as f:\n        f.write(recon_seq)\n\n    accuracy = compute_accuracy(raw_seq, recon_seq)\n    \n    int8_size = latent_int8.nbytes + scale.nbytes\n    int8_ratio = round(int8_size / len(raw_seq), 4)\n\n    quant_results.append({\n        \"file\": fname,\n        \"int8_size_bytes\": int8_size,\n        \"int8_ratio\": int8_ratio,\n        \"int8_acc (%)\": round(accuracy, 2)\n    })\n\n\n\nquant_df = pd.DataFrame(quant_results)\n\ndf_train = pd.read_csv(\"/kaggle/working/evaluation_results.csv\")\ndf_train = df_train.merge(quant_df, on=\"file\", how=\"left\")\ndf_train.to_csv(\"/kaggle/working/evaluation_results.csv\", index=False)\n\ndf_test = pd.read_csv(\"/kaggle/working/evaluation_results_test.csv\")\ndf_test = df_test.merge(quant_df, on=\"file\", how=\"left\")\ndf_test.to_csv(\"/kaggle/working/evaluation_results_test.csv\", index=False)\n\nprint(\"\\n int8 quantization results\")\nprint(df_train.to_string(index=False))\nprint(df_test.to_string(index=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T10:17:07.575423Z","iopub.execute_input":"2025-12-18T10:17:07.575642Z","iopub.status.idle":"2025-12-18T10:17:43.156466Z","shell.execute_reply.started":"2025-12-18T10:17:07.575616Z","shell.execute_reply":"2025-12-18T10:17:43.155882Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Entropy Encoding (Zstd)","metadata":{}},{"cell_type":"code","source":"latent_int8_zstd_dir = \"/kaggle/working/latent_int8_zstd\"\nos.makedirs(latent_int8_zstd_dir, exist_ok=True)\n\n# ZSTD COMPRESSOR \nzstd_compressor = zstd.ZstdCompressor(level=22)\n\ndef compute_int8_blosc_stats(df_subset):\n    int8_blosc_sizes = []\n    int8_blosc_ratios = []\n\n    for row in df_subset.itertuples(index=False):\n        fname = row.file\n\n        latent_fp32 = np.load(os.path.join(latent_dir, f\"{fname}.npy\"))\n\n        min_val, max_val = latent_fp32.min(), latent_fp32.max()\n        scale = (max_val - min_val) / 255 if max_val > min_val else 1.0\n        latent_int8 = np.round((latent_fp32 - min_val) / scale).astype(np.uint8)\n\n        compressed_bytes = zstd_compressor.compress(latent_int8.tobytes())\n        compressed_size = len(compressed_bytes)\n\n        out_path = os.path.join(latent_int8_zstd_dir, f\"{fname}_latent_int8.zst\")\n        with open(out_path, \"wb\") as f:\n            f.write(compressed_bytes)\n\n        ratio = round(compressed_size / row.original_len, 4)\n\n        int8_blosc_sizes.append(compressed_size)\n        int8_blosc_ratios.append(ratio)\n\n    df_subset[\"int8_blosc_size\"] = int8_blosc_sizes\n    df_subset[\"int8_blosc_ratio\"] = int8_blosc_ratios\n\n    return df_subset\n\ndf_train = pd.read_csv(\"/kaggle/working/evaluation_results.csv\")\ndf_test = pd.read_csv(\"/kaggle/working/evaluation_results_test.csv\")\n\ndf_train = compute_int8_blosc_stats(df_train)\ndf_test = compute_int8_blosc_stats(df_test)\n\ndf_train.to_csv(\"/kaggle/working/evaluation_results.csv\", index=False)\ndf_test.to_csv(\"/kaggle/working/evaluation_results_test.csv\", index=False)\n\nprint(\"\\nAdded int8 Blosc stats to both CSVs\")\nprint(\"\\n--- TRAIN ---\")\nprint(df_train.to_string(index=False))\nprint(\"\\n--- TEST ---\")\nprint(df_test.to_string(index=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T10:17:43.15728Z","iopub.execute_input":"2025-12-18T10:17:43.157542Z","iopub.status.idle":"2025-12-18T10:17:43.643816Z","shell.execute_reply.started":"2025-12-18T10:17:43.157525Z","shell.execute_reply":"2025-12-18T10:17:43.64303Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Compute Residual (Predictive Format)","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport zstandard as zstd\nimport struct\n\norig_dir = \"/kaggle/input/dnacorpus/DNACorpus/Prokaryotic\"\nrecon_root = \"/kaggle/working/recon_int8\"\nres_root = \"/kaggle/working/RESIDUALS\"\n\nsubfolders = [\"train\", \"test\"]\n\nfor sf in subfolders:\n    os.makedirs(os.path.join(res_root, sf), exist_ok=True)\n\nBASE2INT = {'A':0, 'T':1, 'C':2, 'G':3, 'N':3}\n\nzstd_compressor = zstd.ZstdCompressor(level=22)\n\ndef pack_2bit(arr):\n    n = len(arr)\n    out = bytearray((n + 3) // 4)\n    for i, v in enumerate(arr):\n        out[i // 4] |= (v & 3) << (6 - 2 * (i % 4))\n    return bytes(out)\n\nfor split in subfolders:\n    recon_dir = os.path.join(recon_root, split)\n    res_out_dir = os.path.join(res_root, split)\n\n    files = sorted(os.listdir(recon_dir))\n\n    print(f\"\\n Processing {split.upper()} ({len(files)} files) \")\n\n    for fname in files:\n\n        if not fname.endswith(\"_recon_int8.txt\"):\n            continue\n\n        base = fname.replace(\"_recon_int8.txt\", \"\")\n        orig_path = os.path.join(orig_dir, base)\n        recon_path = os.path.join(recon_dir, fname)\n\n        if not os.path.exists(orig_path):\n            print(f\"Skipping {fname}: original not found\")\n            continue\n\n        \n        with open(orig_path) as f:\n            original = f.read().strip().upper()\n        with open(recon_path) as f:\n            reconstructed = f.read().strip().upper()\n\n        L = min(len(original), len(reconstructed))\n        original = original[:L]\n        reconstructed = reconstructed[:L]\n\n        \n        orig_i  = np.fromiter((BASE2INT[b] for b in original), dtype=np.uint8)\n        recon_i = np.fromiter((BASE2INT[b] for b in reconstructed), dtype=np.uint8)\n        residual = (orig_i - recon_i) & 3\n\n        packed = pack_2bit(residual)\n        \n        payload = struct.pack(\"<I\", L) + packed\n\n        compressed = zstd_compressor.compress(payload)\n\n        out_path = os.path.join(res_out_dir, base + \".res.zst\")\n        with open(out_path, \"wb\") as f:\n            f.write(compressed)\n\n        print(\n            f\"Saved RES: {out_path} | \"\n            f\"L={L} | \"\n            f\"packed={len(packed)} | \"\n            f\"zstd={len(compressed)} bytes\"\n        )\n\nprint(\"\\n 2-bit packed predictive residuals generated successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T10:17:43.644745Z","iopub.execute_input":"2025-12-18T10:17:43.645068Z","iopub.status.idle":"2025-12-18T10:18:08.345917Z","shell.execute_reply.started":"2025-12-18T10:17:43.645044Z","shell.execute_reply":"2025-12-18T10:18:08.345203Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Final Compressed Size","metadata":{}},{"cell_type":"code","source":"orig_dir        = \"/kaggle/input/dnacorpus/DNACorpus/Prokaryotic\"\nlatent_int8_dir = \"/kaggle/working/latent_int8_zstd\"\nres_root        = \"/kaggle/working/RESIDUALS\"\n\ntrain_csv = \"/kaggle/working/evaluation_results.csv\"\ntest_csv  = \"/kaggle/working/evaluation_results_test.csv\"\n\nsubfolders = [\"train\", \"test\"]\n\ndef get_file_size(path):\n    return os.path.getsize(path) if os.path.exists(path) else 0\n\n\ndef compute_ratios(df, split):\n    results = []\n    res_dir = os.path.join(res_root, split)\n\n    for _, row in df.iterrows():\n        fname = row[\"file\"]\n\n        orig_path = os.path.join(orig_dir, fname)\n        orig_size = get_file_size(orig_path)\n        \n        latent_path = os.path.join(latent_int8_dir, f\"{fname}_latent_int8.zst\")\n        latent_size = get_file_size(latent_path)\n\n        res_path = os.path.join(res_dir, f\"{fname}.res.zst\")\n        residual_size = get_file_size(res_path)\n\n        total_size = latent_size + residual_size\n\n        comp_ratio = round(total_size / orig_size, 6) if orig_size > 0 else 0.0\n\n        results.append({\n            \"file\": fname,\n            \"orig_size_bytes\": orig_size,\n            \"latent_int8_bytes\": latent_size,\n            \"residual_bytes\": residual_size,\n            \"total_output_bytes\": total_size,\n            \"overall_compression_ratio\": comp_ratio\n        })\n\n    return pd.DataFrame(results)\n\n\ndf_train = pd.read_csv(train_csv)\ntrain_stats = compute_ratios(df_train, \"train\")\n\ndf_train[\"file\"] = df_train[\"file\"].astype(str).str.strip()\ntrain_stats[\"file\"] = train_stats[\"file\"].astype(str).str.strip()\n\ndf_train = df_train.merge(train_stats, on=\"file\", how=\"left\")\ndf_train.to_csv(train_csv, index=False)\n\ndf_test = pd.read_csv(test_csv)\ntest_stats = compute_ratios(df_test, \"test\")\n\ndf_test[\"file\"] = df_test[\"file\"].astype(str).str.strip()\ntest_stats[\"file\"] = test_stats[\"file\"].astype(str).str.strip()\n\ndf_test = df_test.merge(test_stats, on=\"file\", how=\"left\")\n\ndf_test.to_csv(test_csv, index=False)\n\nprint(\"\\n Overall compression ratios\")\n\nprint(\"\\n--- TRAIN RESULTS PREVIEW ---\")\ncols = [\n    \"file\",\n    \"orig_size_bytes\",\n    \"latent_int8_bytes\",\n    \"residual_bytes\",\n    \"total_output_bytes\",\n    \"overall_compression_ratio\"\n]\n\nprint(df_train[[c for c in cols if c in df_train.columns]].to_string(index=False))\n\nprint(\"\\n--- TEST RESULTS PREVIEW ---\")\ncols = [\n    \"file\",\n    \"orig_size_bytes\",\n    \"latent_int8_bytes\",\n    \"residual_bytes\",\n    \"total_output_bytes\",\n    \"overall_compression_ratio\"\n]\nprint(df_test[[c for c in cols if c in df_test.columns]].to_string(index=False))\n\n\nplt.figure(figsize=(14, 5))\n\nplot_df = pd.concat([df_train, df_test], ignore_index=True)\n\nx = plot_df[\"file\"]\ny_orig = plot_df[\"orig_size_bytes\"]\ny_comp = plot_df[\"total_output_bytes\"]\n\nplt.plot(x, y_orig, marker='o', label=\"Original Size\")\nplt.plot(x, y_comp, marker='o', label=\"Compressed Size\")\n\nplt.xlabel(\"Files\")\nplt.ylabel(\"Size (bytes)\")\nplt.title(\"Original vs Compressed Size per File\")\nplt.xticks(rotation=45)\nplt.legend()\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T10:18:08.347142Z","iopub.execute_input":"2025-12-18T10:18:08.347352Z","iopub.status.idle":"2025-12-18T10:18:08.579592Z","shell.execute_reply.started":"2025-12-18T10:18:08.347335Z","shell.execute_reply":"2025-12-18T10:18:08.579018Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Verification After Adding Residual","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport zstandard as zstd\nimport struct\n\norig_root  = \"/kaggle/input/dnacorpus/DNACorpus/Prokaryotic\"\nrecon_root = \"/kaggle/working/recon_int8\"\nres_root   = \"/kaggle/working/RESIDUALS\"\n\nBASE2INT = {'A':0, 'T':1, 'C':2, 'G':3, 'N':3}\nINT2BASE = {0:'A', 1:'T', 2:'C', 3:'G'}\n\nsplits = [\"train\", \"test\"]\n\ndef decompress_zst(path):\n    dctx = zstd.ZstdDecompressor()\n    with open(path, \"rb\") as f:\n        return dctx.decompress(f.read())\n\ndef unpack_2bit(packed, L):\n    out = np.empty(L, dtype=np.uint8)\n    idx = 0\n    for b in packed:\n        for shift in (6, 4, 2, 0):\n            if idx < L:\n                out[idx] = (b >> shift) & 3\n                idx += 1\n    return out\n\nresults = []\n\nfor split in splits:\n    recon_dir = os.path.join(recon_root, split)\n    res_dir   = os.path.join(res_root, split)\n\n    print(f\"\\n=== Processing {split.upper()} ===\")\n\n    for fname in sorted(os.listdir(recon_dir)):\n        if not fname.endswith(\"_recon_int8.txt\"):\n            continue\n\n        base = fname.replace(\"_recon_int8.txt\", \"\")\n        recon_path = os.path.join(recon_dir, fname)\n        res_path   = os.path.join(res_dir, base + \".res.zst\")\n        orig_path  = os.path.join(orig_root, base)\n\n        if not os.path.exists(res_path) or not os.path.exists(orig_path):\n            continue\n\n        with open(recon_path) as f:\n            recon = f.read().strip().upper()\n        with open(orig_path) as f:\n            orig = f.read().strip().upper()\n\n        L = min(len(recon), len(orig))\n        recon = recon[:L]\n        orig  = orig[:L]\n\n        raw = decompress_zst(res_path)\n\n        stored_L = struct.unpack(\"<I\", raw[:4])[0]\n        assert stored_L == L\n\n        packed = raw[4:]\n        residual = unpack_2bit(packed, L)\n\n        recon_i = np.fromiter((BASE2INT[b] for b in recon), dtype=np.uint8)\n\n        final_i = (recon_i + residual) & 3\n        final = np.array([INT2BASE[int(x)] for x in final_i])\n\n        mismatches = np.sum(final != np.array(list(orig)))\n        accuracy = 100.0 * (1 - mismatches / L)\n\n        print(f\"{base}: L={L} mism={mismatches} acc={accuracy:.6f}%\")\n        results.append((split, base, L, mismatches, accuracy))\n\ndf = pd.DataFrame(results, columns=[\"Split\",\"File\",\"Length\",\"Mismatches\",\"Accuracy\"])\nprint(\"\\n FINAL ACCURACY \")\nprint(df.to_string(index=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T10:18:08.580351Z","iopub.execute_input":"2025-12-18T10:18:08.580577Z","iopub.status.idle":"2025-12-18T10:18:16.249622Z","shell.execute_reply.started":"2025-12-18T10:18:08.580561Z","shell.execute_reply":"2025-12-18T10:18:16.248932Z"}},"outputs":[],"execution_count":null}]}